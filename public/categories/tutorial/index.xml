<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial on FlammingFrost&#39;s Learning Log</title>
    <link>http://localhost:1313/tech-blog/categories/tutorial/</link>
    <description>Recent content in Tutorial on FlammingFrost&#39;s Learning Log</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Apr 2024 00:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tech-blog/categories/tutorial/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Network Science Notes 1: Random Network</title>
      <link>http://localhost:1313/tech-blog/netsci/netsci/note-1-graph/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/netsci/netsci/note-1-graph/</guid>
      <description>The analysis of network science is basically based on graph theory. We use some quantitative measures to describe the structure of a network and analyze its properties.</description>
    </item>
    <item>
      <title>Network Science Notes 2: Random Network</title>
      <link>http://localhost:1313/tech-blog/netsci/netsci/note-2-randnet/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/netsci/netsci/note-2-randnet/</guid>
      <description>Random networks are the simplest type of network. By studying random networks, I get to know what aspects of a network are important and how to measure them.</description>
    </item>
    <item>
      <title>Network Science Notes 3: Scale-Free Network</title>
      <link>http://localhost:1313/tech-blog/netsci/netsci/note-3-scalefree/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/netsci/netsci/note-3-scalefree/</guid>
      <description>Deviated from the random network, the scale-free network has a power-law degree distribution and seems to be more common in real-world networks. What&amp;rsquo;s the intrinsic mechanism behind it?</description>
    </item>
    <item>
      <title>Network Science Notes 4: Growing Network</title>
      <link>http://localhost:1313/tech-blog/netsci/netsci/note-4-grownet/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/netsci/netsci/note-4-grownet/</guid>
      <description>In this note we generate a network by adding nodes and links one by one (The Barab√°si-Albert Model), this mechanism will help to explain hubs and degree distribution in real-world networks.</description>
    </item>
    <item>
      <title>Basic Concepts in Network Science</title>
      <link>http://localhost:1313/tech-blog/netsci/netsci/basic-concepts/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/netsci/netsci/basic-concepts/</guid>
      <description>Start from graph theory, we use some quantitative measures to describe the structure of a network.</description>
    </item>
    <item>
      <title>Entropy, KL Divergence and Cross Entropy</title>
      <link>http://localhost:1313/tech-blog/notes/ml-intro/cross-entropy/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/notes/ml-intro/cross-entropy/</guid>
      <description>Cross entropy is a loss function used in classification problems. This post will introduce the definition and the intuition behind cross entropy.</description>
    </item>
    <item>
      <title>How to Evaluate the Performance of ML Models</title>
      <link>http://localhost:1313/tech-blog/notes/ml-intro/evaluate/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/notes/ml-intro/evaluate/</guid>
      <description>Maching learning basically have 2 tasks: regression and classification. This post will introduce how to evaluate the performance of ML models in these 2 tasks.</description>
    </item>
    <item>
      <title>Loss Functions</title>
      <link>http://localhost:1313/tech-blog/notes/ml-intro/loss/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/tech-blog/notes/ml-intro/loss/</guid>
      <description>Loss Functions define what a model learns. Different loss casts different focus on the model&amp;rsquo;s learning process.</description>
    </item>
  </channel>
</rss>
